[
  {
    "objectID": "analyse-univarie.html",
    "href": "analyse-univarie.html",
    "title": "Statistique descriptive univariée",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression \n\n\niris=pd.read_csv('iris.csv')\n\nEn appelant le nom du data frame, on peut voir à quoi ressemblent les données contenues dans le fichier.\n\niris\n\n\n\n\n\n\n\n\nSepal_Length\nSepal_Width\nPetal_Length\nPetal_Width\nClass\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns"
  },
  {
    "objectID": "analyse-univarie.html#mesures-de-tendance-et-de-dispersion",
    "href": "analyse-univarie.html#mesures-de-tendance-et-de-dispersion",
    "title": "Statistique descriptive univariée",
    "section": "Mesures de tendance et de dispersion",
    "text": "Mesures de tendance et de dispersion\n\n1 utilisez la fonction describe pour obtenir le résumé quantitatif du jeu de données.\n\niris.describe()\n\n\n\n\n\n\n\n\nSepal_Length\nSepal_Width\nPetal_Length\nPetal_Width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.054000\n3.758667\n1.198667\n\n\nstd\n0.828066\n0.433594\n1.764420\n0.763161\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\n\n2 - Comparez les mesures pour le jeu de données globales et celles obtenues par variété.\n\niris[iris['Class']=='setosa'].describe()\n\n\n\n\n\n\n\n\nSepal_Length\nSepal_Width\nPetal_Length\nPetal_Width\n\n\n\n\ncount\n50.00000\n50.000000\n50.000000\n50.00000\n\n\nmean\n5.00600\n3.418000\n1.464000\n0.24400\n\n\nstd\n0.35249\n0.381024\n0.173511\n0.10721\n\n\nmin\n4.30000\n2.300000\n1.000000\n0.10000\n\n\n25%\n4.80000\n3.125000\n1.400000\n0.20000\n\n\n50%\n5.00000\n3.400000\n1.500000\n0.20000\n\n\n75%\n5.20000\n3.675000\n1.575000\n0.30000\n\n\nmax\n5.80000\n4.400000\n1.900000\n0.60000\n\n\n\n\n\n\n\n\niris[iris['Class']=='versicolor'].describe()\n\n\n\n\n\n\n\n\nSepal_Length\nSepal_Width\nPetal_Length\nPetal_Width\n\n\n\n\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n5.936000\n2.770000\n4.260000\n1.326000\n\n\nstd\n0.516171\n0.313798\n0.469911\n0.197753\n\n\nmin\n4.900000\n2.000000\n3.000000\n1.000000\n\n\n25%\n5.600000\n2.525000\n4.000000\n1.200000\n\n\n50%\n5.900000\n2.800000\n4.350000\n1.300000\n\n\n75%\n6.300000\n3.000000\n4.600000\n1.500000\n\n\nmax\n7.000000\n3.400000\n5.100000\n1.800000\n\n\n\n\n\n\n\n\niris[iris['Class']=='virginica'].describe()\n\n\n\n\n\n\n\n\nSepal_Length\nSepal_Width\nPetal_Length\nPetal_Width\n\n\n\n\ncount\n50.00000\n50.000000\n50.000000\n50.00000\n\n\nmean\n6.58800\n2.974000\n5.552000\n2.02600\n\n\nstd\n0.63588\n0.322497\n0.551895\n0.27465\n\n\nmin\n4.90000\n2.200000\n4.500000\n1.40000\n\n\n25%\n6.22500\n2.800000\n5.100000\n1.80000\n\n\n50%\n6.50000\n3.000000\n5.550000\n2.00000\n\n\n75%\n6.90000\n3.175000\n5.875000\n2.30000\n\n\nmax\n7.90000\n3.800000\n6.900000\n2.50000"
  },
  {
    "objectID": "analyse-univarie.html#visualisation-des-données",
    "href": "analyse-univarie.html#visualisation-des-données",
    "title": "Statistique descriptive univariée",
    "section": "3 - Visualisation des données",
    "text": "3 - Visualisation des données\n\n1 - Tracer la fonction de répartition empirique de la largeur des pétales pour les trois variétés du jeu de données iris, sur trois graphes différents.\n\nsns.ecdfplot(data=iris[iris.Class==\"setosa\"], x='Petal_Width')\n\n\n\n\n\n\n\n\n\nsns.ecdfplot(data=iris[iris.Class==\"virginica\"], x='Petal_Width')\n\n\n\n\n\n\n\n\n\nsns.ecdfplot(data=iris[iris.Class==\"versicolor\"], x='Petal_Width')\n\n\n\n\n\n\n\n\n\n\n2 - Tracer la fonction de répartition empirique de la largeur des pétales pour les trois variétés du jeu de données iris, sur le même graphe, avec l’option hue.\n\nsns.ecdfplot(data=iris, x='Petal_Width', hue=\"Class\")\n\n\n\n\n\n\n\n\n\n\n3 - Pour le jeu de données iris, tracez l’histogramme des longueurs de pétales pour tout le jeu de données.\n\npl = iris.Petal_Length\n\nfig, ax=plt.subplots()\nax.hist(pl,edgecolor='k')\nfig.suptitle('Histogramme des longueurs des pétales')\n\nText(0.5, 0.98, 'Histogramme des longueurs des pétales')\n\n\n\n\n\n\n\n\n\n\nfig, ax=plt.subplots()\nsns.histplot(data=iris, x='Petal_Length')\nfig.suptitle('Histogramme des longueurs des pétales')\n\nText(0.5, 0.98, 'Histogramme des longueurs des pétales')\n\n\n\n\n\n\n\n\n\n\n\n4 - Jouer avec le nombre de classes avec l’option bins.\n\nfig, ax=plt.subplots()\nax.hist(pl,bins=20,edgecolor='k')\nfig.suptitle('Histogramme des longueurs des pétales')\n\nText(0.5, 0.98, 'Histogramme des longueurs des pétales')\n\n\n\n\n\n\n\n\n\n\nfig, ax=plt.subplots()\nsns.histplot(data=iris, x='Petal_Length', bins=20)\nfig.suptitle('Histogramme des longueurs des pétales')\n\nText(0.5, 0.98, 'Histogramme des longueurs des pétales')\n\n\n\n\n\n\n\n\n\n\n\n5 - Pour le jeu de données iris, tracez l’histogramme des longueurs de pétales pour tout le jeu de données en colorant les barres en fonction de l’espèce (option sns hue).\n\nfig, ax=plt.subplots()\nsns.histplot(data=iris, x='Petal_Length', hue='Class')\nfig.suptitle('Histogramme des longueurs des pétales')\n\nText(0.5, 0.98, 'Histogramme des longueurs des pétales')\n\n\n\n\n\n\n\n\n\n\n\n6 - Pour le jeu de données iris, tracer les boîtes à moustache des longueurs de pétales pour chacune des trois variétés.\n\npl1 = pl[iris.Class=='setosa']\npl2 = pl[iris.Class=='versicolor']\npl3 = pl[iris.Class=='virginica']\n\n\nfig, ax=plt.subplots()\nax.boxplot([pl1,pl2,pl3])\nfig.suptitle('Boxplot des longueurs des pétales')\nplt.xticks([1, 2, 3], [\"setosa\", \"versicolor\", \"virginica\"])\n\n([&lt;matplotlib.axis.XTick at 0x1c2cbeda7b0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1c2cbb0a9c0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1c2cbe3f770&gt;],\n [Text(1, 0, 'setosa'), Text(2, 0, 'versicolor'), Text(3, 0, 'virginica')])\n\n\n\n\n\n\n\n\n\n\nfig, ax=plt.subplots()\nsns.boxplot(data=iris, x= 'Class', y='Petal_Length')\nfig.suptitle('Boxplot des longueurs des pétales')\n\nText(0.5, 0.98, 'Boxplot des longueurs des pétales')"
  },
  {
    "objectID": "analyse-bivarie.html",
    "href": "analyse-bivarie.html",
    "title": "Statistique descriptive bivariée",
    "section": "",
    "text": "Le but de ce projet est de s’entraîner à explorer les liaisons entre deux variables."
  },
  {
    "objectID": "analyse-bivarie.html#régression-linéaire-à-la-main",
    "href": "analyse-bivarie.html#régression-linéaire-à-la-main",
    "title": "Statistique descriptive bivariée",
    "section": "1 - Régression linéaire à la main",
    "text": "1 - Régression linéaire à la main\nLe jeu de données gag.csv contient l’âge (en année) et une mesure de concentration en glycosaminoglycane (gag) dans l’urine pour une population d’enfants de 0 à 17 ans.\n\n1 - Importer le jeu de donnée\n\ngag = pd.read_csv('gag.csv')\n\n\n\n2 - Combien y a-t-il d’individus dans ce jeu de données ?\nDimensions du data frame\n\ngag.shape\n\n(314, 2)\n\n\nIl y a 314 individus (lignes).\n\n\n3 - Quel est l’âge moyen des individus ? Quel est leur âge médian ?\n\ngag.describe()\n\n\n\n\n\n\n\n\nAge\nGAG\n\n\n\n\ncount\n314.000000\n314.000000\n\n\nmean\n8.550955\n50.216572\n\n\nstd\n5.158722\n29.043797\n\n\nmin\n0.000000\n0.150561\n\n\n25%\n4.000000\n22.998461\n\n\n50%\n9.000000\n49.190105\n\n\n75%\n13.000000\n74.712759\n\n\nmax\n17.000000\n99.827356\n\n\n\n\n\n\n\nL’âge moyen est 5.28 ans, l’âge médian 4.10 ans.\n\n\n3 - Tracer la boîte à moustaches de l’âge.\n\nfig, ax=plt.subplots()\nax.boxplot(gag.Age)\nfig.suptitle('Boxplot des âges')\n\nText(0.5, 0.98, 'Boxplot des âges')\n\n\n\n\n\n\n\n\n\n\n\n4 - Quel est la concentration moyenne en gag ? Quel est la concentration médiane ?\n\ngag.describe()\n\n\n\n\n\n\n\n\nAge\nGAG\n\n\n\n\ncount\n314.000000\n314.000000\n\n\nmean\n8.550955\n50.216572\n\n\nstd\n5.158722\n29.043797\n\n\nmin\n0.000000\n0.150561\n\n\n25%\n4.000000\n22.998461\n\n\n50%\n9.000000\n49.190105\n\n\n75%\n13.000000\n74.712759\n\n\nmax\n17.000000\n99.827356\n\n\n\n\n\n\n\nLa concentration moyenne est de 13.17, la concentration médiane de 10.6.\n\n\n5 - Tracer la boîte à moustaches de la concentration de gag.\n\nfig, ax=plt.subplots()\nax.boxplot(gag.GAG)\nfig.suptitle('Boxplot de la concentration de gag')\n\nText(0.5, 0.98, 'Boxplot de la concentration de gag')\n\n\n\n\n\n\n\n\n\n\n\n6 - Tracer le nuage de points de la concentration de gag en fonction de l’âge.\n\nfig, ax=plt.subplots()\nax.scatter(gag.Age, gag.GAG, edgecolors='k')\nax.set_xlabel('âge')\nax.set_ylabel('concentration de gag')\n\nText(0, 0.5, 'concentration de gag')\n\n\n\n\n\n\n\n\n\n\n\n7 - Semble-t-il y avoir une relation linéaire entre les variables ?\nIl semble y avoir une liaison décroissante forte, mais pas vraiment linéaire.\n\n\n8 - Calculer les coefficients de la droite de régression de la concentration de gag en fonction de l’âge.\nOn rappelle que la droite de régression de \\(y\\) par rapport à \\(x\\) est la droite d’équation \\(y = \\beta_0 +\\beta_1 x\\) avec\n\\[\n   \\beta_0 = \\bar{y}-\\beta_1 \\bar{x} \\text{,} \\quad \\beta_1 = \\frac{C(x,y)}{V(x)}.\n\\]\n\nC = np.cov(gag.Age, gag.GAG)\nb1 = C[0][1]/C[0][0]\nb0 = np.mean(gag.GAG)-b1*np.mean(gag.Age)\nprint(b0,b1)\n\n51.76076990236237 -0.18058772995835704\n\n\n\n\n9 - Ajoutez la droite de régression sur votre nuage de points.\n\nfig, ax=plt.subplots()\nax.scatter(gag.Age, gag.GAG, edgecolors='k')\nax.plot(gag.Age,b0+b1*gag.Age, color='r')\nax.set_xlabel('âge')\nax.set_ylabel('concentration de gag')\n\nText(0, 0.5, 'concentration de gag')\n\n\n\n\n\n\n\n\n\n\n\n10 - L’ajustement linéaire est-il visuellement satisfaisant ?\nIl semble acceptable pour les valeurs loin de 0, mais très mauvais pour les toutes petites valeurs.\n\n\n11 - Calculez le coefficient de détermination\n\\[\nR^2 = \\frac{SC_reg}{SC_tot} = \\frac{variation expliquée}{variation totale}.\n\\] On rappelle que pour la régression de \\(y\\) par rapport à \\(x\\), on a \\[\nSC_{tot}=\\sum_{k=1}^n(y_k-\\overline y)^2=nV(y),\\quad SC_{reg}=\\sum_{k=1}^n(\\hat y_k-\\overline y)^2,\n\\] avec \\(\\hat y_k = \\beta_0 + \\beta_1 x_k\\). On rappelle également que le coefficient de détermination est égal au coefficient de corrélation au carré.\n\ngag.corr()\n\n\n\n\n\n\n\n\nAge\nGAG\n\n\n\n\nAge\n1.000000\n-0.032076\n\n\nGAG\n-0.032076\n1.000000\n\n\n\n\n\n\n\n\ngag.corr()**2\n\n\n\n\n\n\n\n\nAge\nGAG\n\n\n\n\nAge\n1.000000\n0.001029\n\n\nGAG\n0.001029\n1.000000\n\n\n\n\n\n\n\nOn trouve qu’un peu moins de 50% de la variabilité de la concentration en gag est expliquée par la régression.\nAvec un calcul direct\n\n# somme des carres totale \nSCtot = len(gag.GAG)*np.var(gag.GAG)\n# residus\ngagchap = b0+b1*gag.Age\n# somme des carres expliquee\nSCreg = sum((gagchap-np.mean(gag.GAG))**2)\n# R2\nR2 = SCreg/SCtot\nprint(R2)\n\n0.0010288544093309808\n\n\nOn trouve bien sûr la même chose.\n\n\n12 - L’ajustement linéaire est-il satisfaisant ?\nNon car moins de 50% de la variabilité est expliquée par la régression.\n\n\n13 - Utiliser la droite de régression pour prédire la concentration de gag à 18 ans. Qu’en pensez-vous ?\nOn prédit une valeur négative, ce qui est incohérent. Ceci est dû au mauvais ajustement de la droite de régression."
  },
  {
    "objectID": "analyse-bivarie.html#régression-non-linéaire-à-la-main",
    "href": "analyse-bivarie.html#régression-non-linéaire-à-la-main",
    "title": "Statistique descriptive bivariée",
    "section": "2 - Régression non linéaire à la main",
    "text": "2 - Régression non linéaire à la main\nPour améliorer la qualité de l’ajustement, on peut faire une transformation des données. La visualisation du nuage de points aide à deviner une forme adaptée.\nOn continue à travailler sur le jeu de données gag.csv.\n\n1 - Créer un vecteur lgag égal au logarithme de la concentration en gag.\n\nlgag = np.log(gag.GAG)\n\n\n\n2 - Tracer le nuage de points du logarithme de la concentration de gag en fonction de l’âge.\n\nfig, ax=plt.subplots()\nax.scatter(gag.Age, lgag, edgecolors='k')\nax.set_xlabel('âge')\nax.set_ylabel('log de la concentration de gag')\n\nText(0, 0.5, 'log de la concentration de gag')\n\n\n\n\n\n\n\n\n\n\n\n3 - Calculer les coefficients de la droite de régression du logarithme de la concentration de gag en fonction de l’âge.\n\nC = np.cov(gag.Age, lgag)\nb1 = C[0][1]/C[0][0]\nb0 = np.mean(lgag)-b1*np.mean(gag.Age)\nprint(b0,b1)\n\n3.7389692885116625 -0.01485877679182514\n\n\n\n\n4 - Ajoutez la droite de régression sur votre nuage de points.\n\nfig, ax=plt.subplots()\nax.scatter(gag.Age, lgag, edgecolors='k')\nax.plot(gag.Age,b0+b1*gag.Age, color='r')\nax.set_xlabel('âge')\nax.set_ylabel('log de la concentration de gag')\n\nText(0, 0.5, 'log de la concentration de gag')\n\n\n\n\n\n\n\n\n\n\n\n5 - Evaluer la qualité de l’ajustement linéaire.\nCalculons le coefficient de détermination\n\n# somme des carres totale \nSCtot = len(lgag)*np.var(lgag)\n# residus\ngagchap = b0+b1*gag.Age\n# somme des carres expliquee\nSCreg = sum((gagchap-np.mean(lgag))**2)\n# R2\nR2 = SCreg/SCtot\nprint(R2)\n\n0.0059852483371809475\n\n\nOn trouve maintenant que 72% de la variabilité est expliquée par la régression. C’est beaucoup mieux.\n\n\n6 - Quelle est alors la relation entre l’âge et le logarithme de la concentration de gag fournie par cette droite ? La représenter graphiquement.\nOn a \\(\\log(\\text{gag})=\\beta_0+\\beta_1 \\text{age}\\) ce qui est équivalent à \\(\\text{gag}=\\exp(\\beta_0+\\beta_1 \\text{age})\\).\n\nfig, ax=plt.subplots()\nax.scatter(gag.Age, gag.GAG, edgecolors='k')\nax.plot(gag.Age,np.exp(b0+b1*gag.Age), color='r')\nax.set_xlabel('âge')\nax.set_ylabel('concentration de gag')\n\nText(0, 0.5, 'concentration de gag')\n\n\n\n\n\n\n\n\n\n\n\n7 - Utiliser cette relation pour prédire la concentration de gag à 18 ans.\n\nnp.exp(b0+b1*18)\n\nnp.float64(32.185347624152215)"
  },
  {
    "objectID": "acp.html",
    "href": "acp.html",
    "title": "Analyse en Composantes Principales",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler\n\n\n trees=pd.read_csv('trees.csv')       \n\n\n\n\nscaler = StandardScaler()\nscaler.fit(trees)\nZ = scaler.transform(trees)\nZ\n\narray([[-1.60291968, -0.9572127 , -1.22883711],\n       [-1.50574137, -1.75488995, -1.22883711],\n       [-1.44095583, -2.07396086, -1.23502119],\n       [-0.89027873, -0.6381418 , -0.85160806],\n       [-0.82549319,  0.79767725, -0.70319007],\n       [-0.79310042,  1.11674815, -0.64753332],\n       [-0.72831488, -1.5953545 , -0.90108072],\n       [-0.72831488, -0.15953545, -0.74029457],\n       [-0.69592211,  0.6381418 , -0.46819492],\n       [-0.66352933, -0.15953545, -0.63516516],\n       [-0.63113656,  0.47860635, -0.36924959],\n       [-0.59874379,  0.        , -0.56714024],\n       [-0.59874379,  0.        , -0.54240391],\n       [-0.50156548, -1.11674815, -0.548588  ],\n       [-0.40438717, -0.15953545, -0.68463782],\n       [-0.11285223, -0.3190709 , -0.49293125],\n       [-0.11285223,  1.43581905,  0.22442236],\n       [ 0.01671885,  1.5953545 , -0.17135894],\n       [ 0.14628993, -0.79767725, -0.27648835],\n       [ 0.1786827 , -1.9144254 , -0.32596101],\n       [ 0.24346824,  0.3190709 ,  0.26771094],\n       [ 0.30825379,  0.6381418 ,  0.09455662],\n       [ 0.4054321 , -0.3190709 ,  0.37902443],\n       [ 0.89132366, -0.6381418 ,  0.50270609],\n       [ 0.98850197,  0.15953545,  0.76862165],\n       [ 1.31242968,  0.79767725,  1.56018426],\n       [ 1.37721522,  0.9572127 ,  1.57873651],\n       [ 1.5067863 ,  0.6381418 ,  1.73952266],\n       [ 1.53917907,  0.6381418 ,  1.31900503],\n       [ 1.53917907,  0.6381418 ,  1.28808462],\n       [ 2.38139111,  1.75488995,  2.89594616]])\n\n\n\n\n\n\npca = PCA()\npca.fit(Z)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA() \n\n\n\n\n\n\nnp.cumsum(pca.explained_variance_ratio_)\n\narray([0.80343332, 0.9906391 , 1.        ])\n\n\n\n\n\n\nnp.cumsum(pca.explained_variance_ratio_)\n\narray([0.80343332, 0.9906391 , 1.        ])\n\n\n99% de la variabilité est expliquée par les deux premiers axes, c’est très élevé.\n\n\n\n\nl = 3*pca.explained_variance_ratio_\nl\n\narray([2.41029997, 0.56161733, 0.0280827 ])\n\n\n\n\n\n\nGn=pca.components_\nG=np.empty(shape=Gn.shape)\nfor i in range(0, Gn.shape[0]):\n    G[i,:]=Gn[i,:]*np.sqrt(l[i])\nG\n\narray([[ 0.94481425,  0.75937614,  0.97003808],\n       [-0.30718466,  0.6504939 , -0.21003001],\n       [-0.11385787, -0.01433731,  0.12212093]])\n\n\n\n\n\n\nfig,ax=plt.subplots(figsize=(5,5))\nfor i in range(0, Gn.shape[1]):\n    ax.arrow(0,0,  # la flèche part de l'origine\n             G[0, i],  G[1, i],  # et arrive en (G_1i,G_2i)\n             head_width=0.05,head_length=0.07,length_includes_head=True)\n    ax.text(G[0, i] + 0.01,G[1, i]-0.02, trees.columns[i],fontsize=8)    \n# affichage des lignes horizontales et verticales\nax.plot([-1, 1], [0, 0], color='grey', ls='--')\nax.plot([0, 0], [-1, 1], color='grey', ls='--')\n# nom des axes, avec le pourcentage d'inertie expliqué\nax.set_xlabel('G{} ({}%)'.format(1, round(100*pca.explained_variance_ratio_[0],1)))\nax.set_ylabel('G{} ({}%)'.format(2, round(100*pca.explained_variance_ratio_[1],1)))\nax.set_title(\"Cercle des corrélations (G{} et G{})\".format(1, 2))\nan = np.linspace(0, 2 * np.pi, 100)\nax.plot(np.cos(an), np.sin(an))  \n\n\n\n\n\n\n\n\nExaminer le cercle des corrélation dans le premier plan factoriel pour les variables.\n\n\n\nToutes : les pointes des flèches sont très proches du cercle unité. On le confirme en calulant les cos carrés.\n\nGsq = G**2\nprint(Gsq[0,:]+Gsq[1,:])\n\n[0.98703638 0.99979444 0.98508648]\n\n\n\n\n\nElles sont proches, donc très corrélées positivement.\n\n\n\n\nContrib=(Gn**2)/np.sum(Gn**2,axis=0)\nprint(Contrib[0,:])\n\n[0.37035804 0.23924496 0.390397  ]\n\n\nCe sont les variables de diamlètre et de volume\n\n\n\n\nprint(Contrib[1,:])\n\n[0.16801906 0.75343528 0.07854566]\n\n\nC’est la variable de hauteur.\n\n\n\n\nF=pca.fit_transform(Z)\nF\n\narray([[-2.21148696e+00,  1.70565853e-01,  2.75465135e-01],\n       [-2.54251238e+00, -5.61656700e-01,  2.77685161e-01],\n       [-2.66301584e+00, -8.63434840e-01,  2.56459749e-01],\n       [-1.38602925e+00,  4.96868207e-02,  3.88790636e-02],\n       [-5.51571033e-01,  1.22783579e+00, -1.98227287e-02],\n       [-3.41016327e-01,  1.47591521e+00, -2.85704675e-02],\n       [-1.78657255e+00, -8.33704008e-01, -2.53214499e-02],\n       [-9.83813046e-01,  3.67534336e-01, -3.09929701e-02],\n       [-4.03921851e-01,  9.70386893e-01,  7.70417637e-02],\n       [-8.78699776e-01,  3.11515093e-01,  1.60156561e-03],\n       [-3.80705559e-01,  7.77622994e-01,  1.18778876e-01],\n       [-7.18736993e-01,  4.04372564e-01, -6.49245722e-03],\n       [-7.03281298e-01,  3.97439960e-01,  1.15338181e-02],\n       [-1.19423675e+00, -6.10005058e-01,  3.65455664e-02],\n       [-7.51904891e-01,  2.19157596e-01, -2.10519523e-01],\n       [-5.32736754e-01, -9.25484351e-02, -2.55243849e-01],\n       [ 7.73841983e-01,  1.22966194e+00,  1.17377535e-01],\n       [ 6.83437061e-01,  1.42595005e+00, -2.72726285e-01],\n       [-4.73892315e-01, -6.74864848e-01, -2.32635041e-01],\n       [-1.03132180e+00, -1.64362213e+00, -1.95152140e-01],\n       [ 4.71504197e-01,  1.02129006e-01,  2.37329638e-03],\n       [ 5.58807012e-01,  4.01057173e-01, -1.95126057e-01],\n       [ 3.27489028e-01, -5.49368133e-01,  2.80452814e-02],\n       [ 5.44400656e-01, -1.06015434e+00, -1.84653559e-01],\n       [ 1.15985450e+00, -4.82124308e-01, -1.25142529e-01],\n       [ 2.16370178e+00, -2.82834783e-01,  1.77016028e-01],\n       [ 2.29275317e+00, -1.76112105e-01,  1.32869454e-01],\n       [ 2.31600222e+00, -5.51240998e-01,  1.89304266e-01],\n       [ 2.07296869e+00, -4.46664569e-01, -1.39150982e-01],\n       [ 2.05364908e+00, -4.37998815e-01, -1.61683826e-01],\n       [ 4.11704599e+00, -2.64497206e-01,  3.42257304e-01]])\n\n\n\n\n\n\nfig,ax=plt.subplots(figsize=(5,5))\n# individus\nax.scatter(F[:,0],F[:,1])\nfor i in range(trees.shape[0]):\n    ax.text(F[i,0]+0.1,F[i,1],'{}'.format(i),fontsize=8)\nax.set_xlabel('F{} ({}%)'.format(1, round(100*pca.explained_variance_ratio_[0],1)))\nax.set_ylabel('F{} ({}%)'.format(2, round(100*pca.explained_variance_ratio_[1],1)))\nax.set_title(\"Individus projetés (F{} et F{})\".format(1, 2))\nax.grid()\nax.plot([min(F[:,0]), max(F[:,0])],[0,0], linestyle=\"--\", color='C7')\nax.plot([0, 0],[min(F[:,1]), max(F[:,1])], linestyle=\"--\", color='C7')\n\n\n\n\n\n\n\n\n\n\n\n\ncos2ind = pd.DataFrame(\n    columns=[['axe 1','axe 2','somme']],\n    index=[np.arange(trees.shape[0])])\nfor i in np.arange(trees.shape[0]):\n    for k in np.arange(2):\n        cos2ind.iloc[i,k]= F[i,k]**2/(sum(Z[i,:]**2))\n    cos2ind.iloc[i,2]=cos2ind.iloc[i,0]+cos2ind.iloc[i,1]\ncos2ind\n\n\n\n\n\n\n\n\naxe 1\naxe 2\nsomme\n\n\n\n\n0\n0.978987\n0.005824\n0.984811\n\n\n1\n0.942749\n0.046006\n0.988755\n\n\n2\n0.897343\n0.094334\n0.991678\n\n\n3\n0.997932\n0.001282\n0.999215\n\n\n4\n0.167879\n0.831904\n0.999783\n\n\n5\n0.050662\n0.948982\n0.999644\n\n\n6\n0.821043\n0.178792\n0.999835\n\n\n7\n0.876766\n0.122364\n0.99913\n\n\n8\n0.146887\n0.84777\n0.994656\n\n\n9\n0.888347\n0.11165\n0.999997\n\n\n10\n0.189772\n0.791756\n0.981527\n\n\n11\n0.759522\n0.240416\n0.999938\n\n\n12\n0.757787\n0.242009\n0.999796\n\n\n13\n0.792491\n0.206767\n0.999258\n\n\n14\n0.859591\n0.073026\n0.932617\n\n\n15\n0.793819\n0.023957\n0.817776\n\n\n16\n0.281846\n0.71167\n0.993515\n\n\n17\n0.181407\n0.789706\n0.971112\n\n\n18\n0.305903\n0.620379\n0.926282\n\n\n19\n0.279666\n0.710321\n0.989986\n\n\n20\n0.955163\n0.044813\n0.999976\n\n\n21\n0.610864\n0.314654\n0.925518\n\n\n22\n0.261685\n0.736396\n0.998081\n\n\n23\n0.203777\n0.772779\n0.976556\n\n\n24\n0.844289\n0.145882\n0.990171\n\n\n25\n0.976772\n0.01669\n0.993462\n\n\n26\n0.990826\n0.005846\n0.996672\n\n\n27\n0.94044\n0.053277\n0.993717\n\n\n28\n0.951535\n0.044178\n0.995712\n\n\n29\n0.950854\n0.043252\n0.994106\n\n\n30\n0.989082\n0.004082\n0.993165\n\n\n\n\n\n\n\nTous les individus sont très bien représentés sur ce plan. Seul l’arbre 15 est un peu moins bien reeprésenté que les autres.\n\n\n\nCe sont ceux de plus grand diamètre/volume\n\n\n\nCe sont ceux de plus petit diamètre/volume\n\n\n\nCe sont ceux de plus petite (resp. grande) hauteur.\n\nVous pouvez consulter le pdf"
  },
  {
    "objectID": "acp.html#nous-allons-explorer-le-jeu-de-données-trees.csv-qui-donne-des-mesures-de-diamètre-girth-en-pouces-hauteur-height-en-pieds-et-volume-volume-en-pieds-cubes-de-cerisiers-noirs.",
    "href": "acp.html#nous-allons-explorer-le-jeu-de-données-trees.csv-qui-donne-des-mesures-de-diamètre-girth-en-pouces-hauteur-height-en-pieds-et-volume-volume-en-pieds-cubes-de-cerisiers-noirs.",
    "title": "Analyse en Composantes Principales",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler\n\n\n trees=pd.read_csv('trees.csv')       \n\n\n\n\nscaler = StandardScaler()\nscaler.fit(trees)\nZ = scaler.transform(trees)\nZ\n\narray([[-1.60291968, -0.9572127 , -1.22883711],\n       [-1.50574137, -1.75488995, -1.22883711],\n       [-1.44095583, -2.07396086, -1.23502119],\n       [-0.89027873, -0.6381418 , -0.85160806],\n       [-0.82549319,  0.79767725, -0.70319007],\n       [-0.79310042,  1.11674815, -0.64753332],\n       [-0.72831488, -1.5953545 , -0.90108072],\n       [-0.72831488, -0.15953545, -0.74029457],\n       [-0.69592211,  0.6381418 , -0.46819492],\n       [-0.66352933, -0.15953545, -0.63516516],\n       [-0.63113656,  0.47860635, -0.36924959],\n       [-0.59874379,  0.        , -0.56714024],\n       [-0.59874379,  0.        , -0.54240391],\n       [-0.50156548, -1.11674815, -0.548588  ],\n       [-0.40438717, -0.15953545, -0.68463782],\n       [-0.11285223, -0.3190709 , -0.49293125],\n       [-0.11285223,  1.43581905,  0.22442236],\n       [ 0.01671885,  1.5953545 , -0.17135894],\n       [ 0.14628993, -0.79767725, -0.27648835],\n       [ 0.1786827 , -1.9144254 , -0.32596101],\n       [ 0.24346824,  0.3190709 ,  0.26771094],\n       [ 0.30825379,  0.6381418 ,  0.09455662],\n       [ 0.4054321 , -0.3190709 ,  0.37902443],\n       [ 0.89132366, -0.6381418 ,  0.50270609],\n       [ 0.98850197,  0.15953545,  0.76862165],\n       [ 1.31242968,  0.79767725,  1.56018426],\n       [ 1.37721522,  0.9572127 ,  1.57873651],\n       [ 1.5067863 ,  0.6381418 ,  1.73952266],\n       [ 1.53917907,  0.6381418 ,  1.31900503],\n       [ 1.53917907,  0.6381418 ,  1.28808462],\n       [ 2.38139111,  1.75488995,  2.89594616]])\n\n\n\n\n\n\npca = PCA()\npca.fit(Z)\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA() \n\n\n\n\n\n\nnp.cumsum(pca.explained_variance_ratio_)\n\narray([0.80343332, 0.9906391 , 1.        ])\n\n\n\n\n\n\nnp.cumsum(pca.explained_variance_ratio_)\n\narray([0.80343332, 0.9906391 , 1.        ])\n\n\n99% de la variabilité est expliquée par les deux premiers axes, c’est très élevé.\n\n\n\n\nl = 3*pca.explained_variance_ratio_\nl\n\narray([2.41029997, 0.56161733, 0.0280827 ])\n\n\n\n\n\n\nGn=pca.components_\nG=np.empty(shape=Gn.shape)\nfor i in range(0, Gn.shape[0]):\n    G[i,:]=Gn[i,:]*np.sqrt(l[i])\nG\n\narray([[ 0.94481425,  0.75937614,  0.97003808],\n       [-0.30718466,  0.6504939 , -0.21003001],\n       [-0.11385787, -0.01433731,  0.12212093]])\n\n\n\n\n\n\nfig,ax=plt.subplots(figsize=(5,5))\nfor i in range(0, Gn.shape[1]):\n    ax.arrow(0,0,  # la flèche part de l'origine\n             G[0, i],  G[1, i],  # et arrive en (G_1i,G_2i)\n             head_width=0.05,head_length=0.07,length_includes_head=True)\n    ax.text(G[0, i] + 0.01,G[1, i]-0.02, trees.columns[i],fontsize=8)    \n# affichage des lignes horizontales et verticales\nax.plot([-1, 1], [0, 0], color='grey', ls='--')\nax.plot([0, 0], [-1, 1], color='grey', ls='--')\n# nom des axes, avec le pourcentage d'inertie expliqué\nax.set_xlabel('G{} ({}%)'.format(1, round(100*pca.explained_variance_ratio_[0],1)))\nax.set_ylabel('G{} ({}%)'.format(2, round(100*pca.explained_variance_ratio_[1],1)))\nax.set_title(\"Cercle des corrélations (G{} et G{})\".format(1, 2))\nan = np.linspace(0, 2 * np.pi, 100)\nax.plot(np.cos(an), np.sin(an))  \n\n\n\n\n\n\n\n\nExaminer le cercle des corrélation dans le premier plan factoriel pour les variables.\n\n\n\nToutes : les pointes des flèches sont très proches du cercle unité. On le confirme en calulant les cos carrés.\n\nGsq = G**2\nprint(Gsq[0,:]+Gsq[1,:])\n\n[0.98703638 0.99979444 0.98508648]\n\n\n\n\n\nElles sont proches, donc très corrélées positivement.\n\n\n\n\nContrib=(Gn**2)/np.sum(Gn**2,axis=0)\nprint(Contrib[0,:])\n\n[0.37035804 0.23924496 0.390397  ]\n\n\nCe sont les variables de diamlètre et de volume\n\n\n\n\nprint(Contrib[1,:])\n\n[0.16801906 0.75343528 0.07854566]\n\n\nC’est la variable de hauteur.\n\n\n\n\nF=pca.fit_transform(Z)\nF\n\narray([[-2.21148696e+00,  1.70565853e-01,  2.75465135e-01],\n       [-2.54251238e+00, -5.61656700e-01,  2.77685161e-01],\n       [-2.66301584e+00, -8.63434840e-01,  2.56459749e-01],\n       [-1.38602925e+00,  4.96868207e-02,  3.88790636e-02],\n       [-5.51571033e-01,  1.22783579e+00, -1.98227287e-02],\n       [-3.41016327e-01,  1.47591521e+00, -2.85704675e-02],\n       [-1.78657255e+00, -8.33704008e-01, -2.53214499e-02],\n       [-9.83813046e-01,  3.67534336e-01, -3.09929701e-02],\n       [-4.03921851e-01,  9.70386893e-01,  7.70417637e-02],\n       [-8.78699776e-01,  3.11515093e-01,  1.60156561e-03],\n       [-3.80705559e-01,  7.77622994e-01,  1.18778876e-01],\n       [-7.18736993e-01,  4.04372564e-01, -6.49245722e-03],\n       [-7.03281298e-01,  3.97439960e-01,  1.15338181e-02],\n       [-1.19423675e+00, -6.10005058e-01,  3.65455664e-02],\n       [-7.51904891e-01,  2.19157596e-01, -2.10519523e-01],\n       [-5.32736754e-01, -9.25484351e-02, -2.55243849e-01],\n       [ 7.73841983e-01,  1.22966194e+00,  1.17377535e-01],\n       [ 6.83437061e-01,  1.42595005e+00, -2.72726285e-01],\n       [-4.73892315e-01, -6.74864848e-01, -2.32635041e-01],\n       [-1.03132180e+00, -1.64362213e+00, -1.95152140e-01],\n       [ 4.71504197e-01,  1.02129006e-01,  2.37329638e-03],\n       [ 5.58807012e-01,  4.01057173e-01, -1.95126057e-01],\n       [ 3.27489028e-01, -5.49368133e-01,  2.80452814e-02],\n       [ 5.44400656e-01, -1.06015434e+00, -1.84653559e-01],\n       [ 1.15985450e+00, -4.82124308e-01, -1.25142529e-01],\n       [ 2.16370178e+00, -2.82834783e-01,  1.77016028e-01],\n       [ 2.29275317e+00, -1.76112105e-01,  1.32869454e-01],\n       [ 2.31600222e+00, -5.51240998e-01,  1.89304266e-01],\n       [ 2.07296869e+00, -4.46664569e-01, -1.39150982e-01],\n       [ 2.05364908e+00, -4.37998815e-01, -1.61683826e-01],\n       [ 4.11704599e+00, -2.64497206e-01,  3.42257304e-01]])\n\n\n\n\n\n\nfig,ax=plt.subplots(figsize=(5,5))\n# individus\nax.scatter(F[:,0],F[:,1])\nfor i in range(trees.shape[0]):\n    ax.text(F[i,0]+0.1,F[i,1],'{}'.format(i),fontsize=8)\nax.set_xlabel('F{} ({}%)'.format(1, round(100*pca.explained_variance_ratio_[0],1)))\nax.set_ylabel('F{} ({}%)'.format(2, round(100*pca.explained_variance_ratio_[1],1)))\nax.set_title(\"Individus projetés (F{} et F{})\".format(1, 2))\nax.grid()\nax.plot([min(F[:,0]), max(F[:,0])],[0,0], linestyle=\"--\", color='C7')\nax.plot([0, 0],[min(F[:,1]), max(F[:,1])], linestyle=\"--\", color='C7')\n\n\n\n\n\n\n\n\n\n\n\n\ncos2ind = pd.DataFrame(\n    columns=[['axe 1','axe 2','somme']],\n    index=[np.arange(trees.shape[0])])\nfor i in np.arange(trees.shape[0]):\n    for k in np.arange(2):\n        cos2ind.iloc[i,k]= F[i,k]**2/(sum(Z[i,:]**2))\n    cos2ind.iloc[i,2]=cos2ind.iloc[i,0]+cos2ind.iloc[i,1]\ncos2ind\n\n\n\n\n\n\n\n\naxe 1\naxe 2\nsomme\n\n\n\n\n0\n0.978987\n0.005824\n0.984811\n\n\n1\n0.942749\n0.046006\n0.988755\n\n\n2\n0.897343\n0.094334\n0.991678\n\n\n3\n0.997932\n0.001282\n0.999215\n\n\n4\n0.167879\n0.831904\n0.999783\n\n\n5\n0.050662\n0.948982\n0.999644\n\n\n6\n0.821043\n0.178792\n0.999835\n\n\n7\n0.876766\n0.122364\n0.99913\n\n\n8\n0.146887\n0.84777\n0.994656\n\n\n9\n0.888347\n0.11165\n0.999997\n\n\n10\n0.189772\n0.791756\n0.981527\n\n\n11\n0.759522\n0.240416\n0.999938\n\n\n12\n0.757787\n0.242009\n0.999796\n\n\n13\n0.792491\n0.206767\n0.999258\n\n\n14\n0.859591\n0.073026\n0.932617\n\n\n15\n0.793819\n0.023957\n0.817776\n\n\n16\n0.281846\n0.71167\n0.993515\n\n\n17\n0.181407\n0.789706\n0.971112\n\n\n18\n0.305903\n0.620379\n0.926282\n\n\n19\n0.279666\n0.710321\n0.989986\n\n\n20\n0.955163\n0.044813\n0.999976\n\n\n21\n0.610864\n0.314654\n0.925518\n\n\n22\n0.261685\n0.736396\n0.998081\n\n\n23\n0.203777\n0.772779\n0.976556\n\n\n24\n0.844289\n0.145882\n0.990171\n\n\n25\n0.976772\n0.01669\n0.993462\n\n\n26\n0.990826\n0.005846\n0.996672\n\n\n27\n0.94044\n0.053277\n0.993717\n\n\n28\n0.951535\n0.044178\n0.995712\n\n\n29\n0.950854\n0.043252\n0.994106\n\n\n30\n0.989082\n0.004082\n0.993165\n\n\n\n\n\n\n\nTous les individus sont très bien représentés sur ce plan. Seul l’arbre 15 est un peu moins bien reeprésenté que les autres.\n\n\n\nCe sont ceux de plus grand diamètre/volume\n\n\n\nCe sont ceux de plus petit diamètre/volume\n\n\n\nCe sont ceux de plus petite (resp. grande) hauteur.\n\nVous pouvez consulter le pdf"
  }
]